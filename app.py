import streamlit as st
import uuid
from db import save_message, load_messages
from chatbot import generate_response as generate_response_gemini
from local_model import LocalLLM  # local_model.pyì—ì„œ LocalLLM í´ë˜ìŠ¤ import
import os

st.set_page_config(page_title="Woosong Chatbot", page_icon="ğŸ“", layout="wide")

# --- ëª¨ë¸ ì´ˆê¸°í™” ---
@st.cache_resource  # ì´ ë°ì½”ë ˆì´í„°ëŠ” ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œí•©ë‹ˆë‹¤
def load_local_model():
    return LocalLLM()

# --- ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„ íƒ ì˜µì…˜ ì¶”ê°€ ---
with st.sidebar:
    st.title("Model Settings")
    model_option = st.selectbox(
        "Select AI Model",
        ["Gemini-1.5-Flash", "Local-Qwen-0.5B"],
        help="Select the AI model to use for generating responses"
    )
    
    if model_option == "Local-Qwen-0.5B":
        try:
            if 'local_model' not in st.session_state:
                with st.spinner("Loading local model..."):
                    st.session_state.local_model = load_local_model()
            st.success("Local model loaded successfully!")
        except Exception as e:
            st.error(f"Model loading failed: {str(e)}")
            st.warning("Switching back to Gemini model.")
            model_option = "Gemini-1.5-Flash"

st.title("ğŸ“ Woosong University Student Chatbot")
st.caption("Hello! I'm an AI chatbot for Woosong University students. Ask me anything!")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

if "messages" not in st.session_state:
    # ì„¸ì…˜ ì‹œì‘ ì‹œ DBì—ì„œ ì´ì „ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
    st.session_state.messages = load_messages(st.session_state.session_id)
    # DBì— ê¸°ë¡ì´ ì—†ìœ¼ë©´ ì´ˆê¸° ë©”ì‹œì§€ ì¶”ê°€ (í™”ë©´ í‘œì‹œìš©)
    if not st.session_state.messages:
        st.session_state.messages = [{'role': 'assistant', 'content': "Hello! How can I help you today?"}]

# --- ëŒ€í™” ë‚´ìš© í‘œì‹œ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ---
prompt = st.chat_input("Enter your message here...")

if prompt:
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  DBì— ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    save_message(st.session_state.session_id, "user", prompt)

    # 2. ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‘ë‹µ ìƒì„±
    with st.spinner("Generating response..."):
        if model_option == "Gemini-1.5-Flash":
            current_history = load_messages(st.session_state.session_id)
            full_response = generate_response_gemini(current_history)
        else:  # Local-Qwen-0.5B
            # ë¡œì»¬ ëª¨ë¸ì€ í˜„ì¬ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬
            full_response = st.session_state.local_model.generate_response(prompt)

    if full_response:
        # 3. ì‘ë‹µì„ í™”ë©´ì— í‘œì‹œí•˜ê³  DBì— ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        with st.chat_message("assistant"):
            st.markdown(full_response)
        save_message(st.session_state.session_id, "assistant", full_response)
    else:
        # ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆí•„ìš” ì‹œ
        st.warning("Sorry, I couldn't generate a response.")

# --- ëª¨ë¸ ìƒíƒœ í‘œì‹œ ---
with st.sidebar:
    st.divider()
    st.write("Currently using model:", model_option)
    if model_option == "Local-Qwen-0.5B":
        st.write("Model Status: Loaded")
        st.write("Device:", st.session_state.local_model.device if 'local_model' in st.session_state else "Not loaded")
